import json
import os
from urllib.parse import urljoin
from playwright.sync_api import Page, TimeoutError
from src.config.settings import FILE_PATHS, SCRAPER_CONFIG

INPUT_FILE = FILE_PATHS["month_document_urls"]
OUTPUT_FILE = FILE_PATHS["month_document_contents"]

def extract_field_from_table(page: Page, label: str) -> str:
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å table row <tr> ‡∏ó‡∏µ‡πà‡∏°‡∏µ label"""
    try:
        row = page.locator(f"xpath=//tr[td/strong[contains(normalize-space(), '{label}')]]").first
        if row.count() == 0:
            return ""
        content = row.locator("td").nth(1).inner_text().strip()
        if content.startswith(":"):
            content = content[1:].strip()
        return content
    except Exception as e:
        print(f"‚ö†Ô∏è ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• '{label}' ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e}")
        return ""

def read_documents_from_table_list(page: Page) -> list[dict]:
    """‡∏≠‡πà‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤ list ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô table"""
    results = []
    container = page.locator("div[id^='c'] table tbody")
    topic_rows = container.locator("xpath=.//tr[td//span[contains(normalize-space(), '‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á')]]")
    count = topic_rows.count()
    print(f"üìå ‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡∏´‡∏ô‡πâ‡∏≤ list ‡∏°‡∏µ {count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")

    for i in range(count):
        row = topic_rows.nth(i)
        link = row.locator("a").first
        title = link.inner_text().strip()
        href = link.get_attribute("href")
        url = urljoin(page.url, href)

        detail_row = row.locator("xpath=following-sibling::tr[1]")
        detail_text = detail_row.inner_text()

        def extract(label: str) -> str:
            if label in detail_text:
                return detail_text.split(label, 1)[1].strip().split("\n")[0]
            return ""

        results.append({
            "title": title,
            "url": url,
            "‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠": extract("‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠"),
            "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà": extract("‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà"),
        })
    return results

def read_single_document(page: Page, url: str, fallback_title: str):
    """‡∏≠‡πà‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ 1 URL (auto detect ‡∏´‡∏ô‡πâ‡∏≤ list / detail)"""
    try:
        page.goto(url, wait_until="domcontentloaded", timeout=SCRAPER_CONFIG["page_timeout"] * 1.5)
    except TimeoutError:
        print(f"‚ö†Ô∏è ‡πÇ‡∏´‡∏•‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö timeout: {url}")
        return {"title": fallback_title, "url": url, "error": "timeout"}

    if page.locator("span:has-text('‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á')").count() > 1:
        print("üìÑ ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡πâ‡∏≤ list ‡πÅ‡∏ö‡∏ö table")
        return read_documents_from_table_list(page)

    return {
        "title": fallback_title,
        "url": url,
        "‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠": extract_field_from_table(page, "‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠"),
        "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà": extract_field_from_table(page, "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà"),
        "‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á": extract_field_from_table(page, "‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á"),
        "‡∏Ç‡πâ‡∏≠‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢": extract_field_from_table(page, "‡∏Ç‡πâ‡∏≠‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢"),
        "‡∏Ç‡πâ‡∏≠‡∏´‡∏≤‡∏£‡∏∑‡∏≠": extract_field_from_table(page, "‡∏Ç‡πâ‡∏≠‡∏´‡∏≤‡∏£‡∏∑‡∏≠"),
        "‡πÅ‡∏ô‡∏ß‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢": extract_field_from_table(page, "‡πÅ‡∏ô‡∏ß‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢"),
    }

def run_read_document_content(page: Page):
    """Main task: ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏î‡∏∑‡∏≠‡∏ô"""
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)

    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        months = json.load(f)

    results = []
    total_links = 0
    total_documents = 0

    for m in months:
        print(f"\nüìÖ ‡∏õ‡∏µ {m['year']} ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô {m['month']}")
        documents = []
        month_documents_count = 0

        for item in m.get("links", []):
            total_links += 1
            print(f"   üîó {item['url']}")
            data = read_single_document(page, item["url"], item.get("title", ""))

            if isinstance(data, list):
                documents.extend(data)
                month_documents_count += len(data)
                total_documents += len(data)
            else:
                documents.append(data)
                month_documents_count += 1
                total_documents += 1

        print(f"   ‚úÖ ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ {month_documents_count} ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£")
        results.append({
            "year": m["year"],
            "month": m["month"],
            "month_no": m.get("month_no", ""),
            "documents": documents
        })

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print("\nüìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î")
    print(f"üîó URL ‡∏ó‡∏µ‡πà‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î : {total_links}")
    print(f"üìÑ ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á    : {total_documents}")
    print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏µ‡πà        : {OUTPUT_FILE}")

if __name__ == "__main__":
    from playwright.sync_api import sync_playwright
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        page = browser.new_page()
        run_read_document_content(page)
        browser.close()
